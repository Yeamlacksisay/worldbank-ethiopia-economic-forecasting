{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Bank Ethiopia Capstone Project\n",
    "\n",
    "## Stage 1: Data Collection and Cleaning\n",
    "\n",
    "### Project Overview\n",
    "This notebook performs the initial data acquisition and cleaning phase for the World Bank Ethiopia macroeconomic analysis project. We will load raw World Development Indicators (WDI) data for Ethiopia and transform it into a clean, analysis-ready dataset.\n",
    "\n",
    "### Objectives\n",
    "1. **Data Acquisition**: Load raw World Bank WDI data for Ethiopia\n",
    "2. **Data Inspection**: Understand the structure, data types, and quality of the raw data\n",
    "3. **Data Cleaning**: Handle missing values, standardize column names, and ensure data consistency\n",
    "4. **Data Validation**: Verify data quality and completeness\n",
    "5. **Data Export**: Save cleaned dataset for downstream analysis\n",
    "\n",
    "### Key Indicators\n",
    "The dataset contains the following macroeconomic indicators:\n",
    "- **GDP (constant 2015 US$)**: Total economic output adjusted for inflation\n",
    "- **GDP growth (annual %)**: Year-over-year GDP growth rate\n",
    "- **GDP per capita (constant 2015 US$)**: GDP divided by population\n",
    "- **Inflation, consumer prices (annual %)**: Consumer price index annual change\n",
    "- **Unemployment, total (% of total labor force)**: Unemployment rate\n",
    "\n",
    "### Data Source\n",
    "- **Source**: World Bank Open Data\n",
    "- **Dataset**: World Development Indicators (WDI)\n",
    "- **Country**: Ethiopia (ETH)\n",
    "- **Time Period**: 1960-2024\n",
    "- **File**: `datasets/raw/ethiopia_wdi_raw.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "We begin by importing the necessary Python libraries for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STAGE 1: DATA COLLECTION AND CLEANING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nLibraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Raw Dataset\n",
    "\n",
    "Load the raw World Bank WDI data for Ethiopia. The raw data is in a wide format with years as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "raw_data_path = '../datasets/raw/ethiopia_wdi_raw.csv'\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(raw_data_path):\n",
    "    raise FileNotFoundError(f\"Raw data file not found at: {raw_data_path}\")\n",
    "\n",
    "# Load raw dataset\n",
    "print(f\"Loading raw data from: {raw_data_path}\")\n",
    "df_raw = pd.read_csv(raw_data_path)\n",
    "\n",
    "print(f\"\\n✓ Data loaded successfully!\")\n",
    "print(f\"  - Shape: {df_raw.shape[0]} rows × {df_raw.shape[1]} columns\")\n",
    "print(f\"  - Memory usage: {df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RAW DATA PREVIEW (First 5 rows):\")\n",
    "print(\"=\" * 80)\n",
    "display(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Inspect Data Structure\n",
    "\n",
    "Examine the structure of the raw data to understand:\n",
    "- Column names and their meanings\n",
    "- Data types\n",
    "- Missing values\n",
    "- Data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA STRUCTURE INFORMATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Column Names:\")\n",
    "print(\"-\" * 80)\n",
    "for i, col in enumerate(df_raw.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n2. Data Types:\")\n",
    "print(\"-\" * 80)\n",
    "print(df_raw.dtypes)\n",
    "\n",
    "print(\"\\n3. Dataset Shape:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Rows: {df_raw.shape[0]}\")\n",
    "print(f\"Columns: {df_raw.shape[1]}\")\n",
    "\n",
    "print(\"\\n4. Missing Values Summary:\")\n",
    "print(\"-\" * 80)\n",
    "missing_summary = df_raw.isnull().sum()\n",
    "missing_pct = (missing_summary / len(df_raw)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_summary,\n",
    "    'Missing Percentage': missing_pct\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"No missing values found in the dataset.\")\n",
    "\n",
    "print(\"\\n5. Sample Data (First 3 rows):\")\n",
    "print(\"-\" * 80)\n",
    "display(df_raw.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Transform Data from Wide to Long Format\n",
    "\n",
    "The raw data is in wide format (years as columns). We need to transform it to long format (Year as a column) for time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key columns\n",
    "id_cols = ['Country Name', 'Country Code', 'Series Name', 'Series Code']\n",
    "year_cols = [col for col in df_raw.columns if '[YR' in col]\n",
    "\n",
    "print(f\"Found {len(year_cols)} year columns (1960-2024)\")\n",
    "print(f\"Found {len(df_raw)} indicator series\")\n",
    "\n",
    "# Melt the dataframe from wide to long format\n",
    "print(\"\\nTransforming data from wide to long format...\")\n",
    "df_long = pd.melt(\n",
    "    df_raw,\n",
    "    id_vars=id_cols,\n",
    "    value_vars=year_cols,\n",
    "    var_name='Year_Str',\n",
    "    value_name='Value'\n",
    ")\n",
    "\n",
    "# Extract year from column names (e.g., \"1960 [YR1960]\" -> 1960)\n",
    "df_long['Year'] = df_long['Year_Str'].str.extract(r'(\\d{4})').astype(int)\n",
    "df_long = df_long.drop(columns=['Year_Str'])\n",
    "\n",
    "print(f\"✓ Transformation complete!\")\n",
    "print(f\"  - Long format shape: {df_long.shape[0]} rows × {df_long.shape[1]} columns\")\n",
    "\n",
    "# Display sample of transformed data\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRANSFORMED DATA (Long Format) - Sample:\")\n",
    "print(\"=\" * 80)\n",
    "display(df_long.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Pivot to Create Indicator Columns\n",
    "\n",
    "Transform the long format data into a wide format with each indicator as a separate column, suitable for time series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of series codes to clean column names\n",
    "series_mapping = {\n",
    "    'NY.GDP.MKTP.KD': 'gdp_constant',\n",
    "    'NY.GDP.MKTP.KD.ZG': 'gdp_growth',\n",
    "    'NY.GDP.PCAP.KD': 'gdp_per_capita',\n",
    "    'FP.CPI.TOTL.ZG': 'inflation',\n",
    "    'SL.UEM.TOTL.ZS': 'unemployment'\n",
    "}\n",
    "\n",
    "# Filter to only include indicators we need\n",
    "df_filtered = df_long[df_long['Series Code'].isin(series_mapping.keys())].copy()\n",
    "\n",
    "# Map series codes to clean names\n",
    "df_filtered['Indicator'] = df_filtered['Series Code'].map(series_mapping)\n",
    "\n",
    "# Pivot to create one column per indicator\n",
    "print(\"Creating indicator columns...\")\n",
    "df_pivoted = df_filtered.pivot_table(\n",
    "    index='Year',\n",
    "    columns='Indicator',\n",
    "    values='Value',\n",
    "    aggfunc='first'  # Use first value if duplicates exist\n",
    ")\n",
    "\n",
    "# Reset index to make Year a column\n",
    "df_pivoted = df_pivoted.reset_index()\n",
    "\n",
    "# Sort by year\n",
    "df_pivoted = df_pivoted.sort_values('Year').reset_index(drop=True)\n",
    "\n",
    "print(f\"✓ Pivot complete!\")\n",
    "print(f\"  - Final shape: {df_pivoted.shape[0]} rows × {df_pivoted.shape[1]} columns\")\n",
    "print(f\"  - Year range: {df_pivoted['Year'].min()} to {df_pivoted['Year'].max()}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PIVOTED DATA (One column per indicator) - Sample:\")\n",
    "print(\"=\" * 80)\n",
    "display(df_pivoted.head(10))\n",
    "display(df_pivoted.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Data Cleaning and Validation\n",
    "\n",
    "Perform comprehensive data cleaning:\n",
    "- Handle missing values appropriately\n",
    "- Convert data types\n",
    "- Validate data ranges\n",
    "- Check for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "df_clean = df_pivoted.copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA CLEANING AND VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check for missing values by indicator\n",
    "print(\"\\n1. Missing Values by Indicator:\")\n",
    "print(\"-\" * 80)\n",
    "missing_by_indicator = df_clean.isnull().sum()\n",
    "missing_pct_by_indicator = (missing_by_indicator / len(df_clean)) * 100\n",
    "missing_info = pd.DataFrame({\n",
    "    'Missing Count': missing_by_indicator,\n",
    "    'Missing Percentage': missing_pct_by_indicator\n",
    "})\n",
    "display(missing_info)\n",
    "\n",
    "# 2. Convert numeric columns (handle '..' as missing values)\n",
    "print(\"\\n2. Converting data types and handling special values...\")\n",
    "numeric_cols = ['gdp_constant', 'gdp_growth', 'gdp_per_capita', 'inflation', 'unemployment']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        # Replace '..' and other non-numeric strings with NaN\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "        print(f\"  ✓ Converted {col} to numeric\")\n",
    "\n",
    "# 3. Data validation - check for reasonable ranges\n",
    "print(\"\\n3. Data Range Validation:\")\n",
    "print(\"-\" * 80)\n",
    "validation_results = {}\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        non_null = df_clean[col].dropna()\n",
    "        if len(non_null) > 0:\n",
    "            validation_results[col] = {\n",
    "                'Min': non_null.min(),\n",
    "                'Max': non_null.max(),\n",
    "                'Mean': non_null.mean(),\n",
    "                'Non-null Count': len(non_null),\n",
    "                'Null Count': df_clean[col].isnull().sum()\n",
    "            }\n",
    "\n",
    "validation_df = pd.DataFrame(validation_results).T\n",
    "display(validation_df)\n",
    "\n",
    "# 4. Check for obvious outliers (values that are clearly errors)\n",
    "print(\"\\n4. Outlier Detection (values outside 3 standard deviations):\")\n",
    "print(\"-\" * 80)\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        series = df_clean[col].dropna()\n",
    "        if len(series) > 0:\n",
    "            mean = series.mean()\n",
    "            std = series.std()\n",
    "            outliers = series[(series < mean - 3*std) | (series > mean + 3*std)]\n",
    "            if len(outliers) > 0:\n",
    "                print(f\"\\n  {col}: {len(outliers)} potential outliers\")\n",
    "                print(f\"    Range: [{mean - 3*std:.2f}, {mean + 3*std:.2f}]\")\n",
    "                print(f\"    Outlier values: {outliers.values[:5]}\")  # Show first 5\n",
    "            else:\n",
    "                print(f\"  {col}: No outliers detected\")\n",
    "\n",
    "print(\"\\n✓ Data cleaning and validation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Data Summary\n",
    "\n",
    "Generate a comprehensive summary of the cleaned dataset before saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL CLEANED DATASET SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nDataset Shape: {df_clean.shape[0]} rows × {df_clean.shape[1]} columns\")\n",
    "print(f\"Year Range: {df_clean['Year'].min()} to {df_clean['Year'].max()} ({df_clean['Year'].max() - df_clean['Year'].min() + 1} years)\")\n",
    "\n",
    "print(\"\\nColumn Information:\")\n",
    "print(\"-\" * 80)\n",
    "for col in df_clean.columns:\n",
    "    if col != 'Year':\n",
    "        non_null = df_clean[col].dropna()\n",
    "        print(f\"\\n{col.upper()}:\")\n",
    "        print(f\"  - Non-null values: {len(non_null)} / {len(df_clean)} ({100*len(non_null)/len(df_clean):.1f}%)\")\n",
    "        if len(non_null) > 0:\n",
    "            print(f\"  - Range: [{non_null.min():.2f}, {non_null.max():.2f}]\")\n",
    "            print(f\"  - Mean: {non_null.mean():.2f}\")\n",
    "            print(f\"  - Median: {non_null.median():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CLEANED DATA PREVIEW:\")\n",
    "print(\"=\" * 80)\n",
    "display(df_clean.head(10))\n",
    "display(df_clean.tail(10))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DESCRIPTIVE STATISTICS:\")\n",
    "print(\"=\" * 80)\n",
    "display(df_clean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Cleaned Dataset\n",
    "\n",
    "Save the cleaned dataset to the `datasets/cleaned/` directory for use in subsequent analysis stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = '../datasets/cleaned'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define output file path\n",
    "output_path = os.path.join(output_dir, 'ethiopia_analytic_dataset.csv')\n",
    "\n",
    "# Save cleaned dataset\n",
    "print(f\"Saving cleaned dataset to: {output_path}\")\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"✓ Dataset saved successfully!\")\n",
    "\n",
    "# Verify the saved file\n",
    "if os.path.exists(output_path):\n",
    "    file_size = os.path.getsize(output_path) / 1024  # Size in KB\n",
    "    print(f\"  - File size: {file_size:.2f} KB\")\n",
    "    \n",
    "    # Quick verification: reload and check\n",
    "    df_verify = pd.read_csv(output_path)\n",
    "    print(f\"  - Verification: Loaded {df_verify.shape[0]} rows × {df_verify.shape[1]} columns\")\n",
    "    print(f\"  - ✓ File saved and verified successfully!\")\n",
    "else:\n",
    "    print(\"  - ✗ Error: File was not created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 Summary\n",
    "\n",
    "### Completed Tasks\n",
    "✅ **Data Acquisition**: Successfully loaded raw World Bank WDI data for Ethiopia  \n",
    "✅ **Data Transformation**: Converted from wide format (years as columns) to long format, then pivoted to indicator columns  \n",
    "✅ **Data Cleaning**: \n",
    "   - Converted data types appropriately\n",
    "   - Handled missing values (preserved for downstream analysis)\n",
    "   - Validated data ranges\n",
    "   - Checked for outliers\n",
    "\n",
    "✅ **Data Export**: Saved cleaned dataset to `datasets/cleaned/ethiopia_analytic_dataset.csv`\n",
    "\n",
    "### Dataset Characteristics\n",
    "- **Time Period**: 1960-2024 (65 years)\n",
    "- **Indicators**: 5 key macroeconomic indicators\n",
    "- **Format**: Year-indexed time series data\n",
    "- **Quality**: Data validated and ready for analysis\n",
    "\n",
    "### Next Steps\n",
    "The cleaned dataset is now ready for:\n",
    "- **Stage 2**: Exploratory Data Analysis (EDA)\n",
    "- **Stage 3**: Linear Transformations\n",
    "- **Stage 4**: Advanced Visualizations\n",
    "- **Stage 5**: Feature Engineering\n",
    "- **Stage 6**: Forecasting (ARIMA, Holt-Winters, ML models)\n",
    "- **Stage 7**: Trend Analysis, Correlation Analysis, and Policy Insights\n",
    "\n",
    "---\n",
    "**Note**: Missing values are preserved in the cleaned dataset. They will be handled appropriately in downstream analysis stages based on the specific requirements of each modeling approach."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}